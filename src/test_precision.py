#!/usr/bin/env python3
"""
æ•™è®­æå–ç®—æ³• - å®Œæ•´è¯„ä¼°ç‰ˆ
åŒ…å«ï¼šå¬å›ç‡ + ç²¾ç¡®ç‡ + F1 + è¯¯æŠ¥ç‡ + è¯„åˆ†åˆ†å¸ƒ
"""

import json
import re
import os
from datetime import datetime
from typing import List, Dict, Any, Tuple

class LessonExtractor:
    def __init__(self, time_decay_factor: float = 0.1, threshold: float = 0.3):
        self.time_decay = time_decay_factor
        self.threshold = threshold

    def semantic_similarity(self, trigger: str, lesson: str) -> float:
        """å…³é”®è¯é‡å åº¦"""
        trigger_words = set(trigger.lower().split())
        lesson_words = set(lesson.lower().split())
        overlap = len(trigger_words & lesson_words)
        return min(overlap / max(len(trigger_words), 1) * 2, 1.0)

    def calculate_score(self, trigger: str, lesson: str,
                       frequency: int, cost_factor: float) -> float:
        """å¤åˆè¯„åˆ†"""
        semantic_weight = self.semantic_similarity(trigger, lesson)
        frequency_score = min(frequency * 0.5, 3.0)
        cost_score = min(cost_factor * 0.3, 2.0)

        return (semantic_weight * 0.4 +
                frequency_score * 0.3 +
                cost_score * 0.2 +
                self.time_decay * 0.1)

    def extract_from_logs(self, log_text: str,
                         lessons: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»æ—¥å¿—ä¸­æå–å€™é€‰æ•™è®­äº‹ä»¶"""
        candidates = []
        for lesson in lessons:
            trigger = lesson.get('trigger', '')
            # ç®€å•åŒ¹é…ï¼šè§¦å‘è¯å‡ºç°åœ¨æ—¥å¿—ä¸­
            if re.search(re.escape(trigger), log_text, re.IGNORECASE):
                score = self.calculate_score(
                    trigger=trigger,
                    lesson=lesson.get('lesson', ''),
                    frequency=2,
                    cost_factor=1.5
                )
                if score >= self.threshold:
                    candidates.append({
                        'id': lesson.get('id'),
                        'trigger': trigger,
                        'score': round(score, 3),
                        'lesson_id': lesson.get('id')
                    })
        return candidates

    def load_lessons(self, filepath: str) -> List[Dict[str, Any]]:
        """åŠ è½½ç§å­æ•™è®­"""
        lessons = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    lessons.append(json.loads(line))
        return lessons

    def evaluate_full(self, candidates: List[Dict[str, Any]],
                     ground_truth: List[str]) -> Dict[str, Any]:
        """
        å®Œæ•´è¯„ä¼°
        ground_truth: çœŸå®æ•™è®­ ID åˆ—è¡¨
        """
        # æå–å€™é€‰ ID
        candidate_ids = [c['lesson_id'] for c in candidates]

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        tp = len(set(candidate_ids) & set(ground_truth))  # çœŸæ­£ä¾‹
        fp = len(set(candidate_ids) - set(ground_truth))  # å‡æ­£ä¾‹
        fn = len(set(ground_truth) - set(candidate_ids))  # å‡è´Ÿä¾‹

        # å¬å›ç‡ã€ç²¾ç¡®ç‡ã€F1
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # è¯¯æŠ¥ç‡
        false_positive_rate = fp / len(candidates) if candidates else 0

        # è¯„åˆ†åˆ†å¸ƒ
        scores = [c['score'] for c in candidates]
        score_distribution = {
            'min': min(scores) if scores else 0,
            'max': max(scores) if scores else 0,
            'avg': sum(scores) / len(scores) if scores else 0,
            'count': len(scores)
        }

        return {
            'metrics': {
                'recall': round(recall, 3),
                'precision': round(precision, 3),
                'f1': round(f1, 3),
                'false_positive_rate': round(false_positive_rate, 3),
                'tp': tp,
                'fp': fp,
                'fn': fn
            },
            'score_distribution': score_distribution,
            'candidates': candidates
        }

def main():
    extractor = LessonExtractor(threshold=0.3)

    # åŠ è½½ç§å­æ•™è®­
    lessons = extractor.load_lessons('memory/lessons.jsonl')
    ground_truth = [l['id'] for l in lessons]

    print("ğŸ“Š Memory Lab - å®Œæ•´è¯„ä¼°")
    print("=" * 60)
    print(f"ç§å­æ•°æ®ï¼š{len(lessons)} æ¡")
    print(f"é˜ˆå€¼ï¼š{extractor.threshold}\n")

    # æ¨¡æ‹Ÿæµ‹è¯•ï¼šç”¨ç§å­æ•°æ®ä½œä¸ºæ—¥å¿—ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
    # å®é™… Phase 2 ä¼šç”¨çœŸå® 30 å¤©æ—¥å¿—
    test_log = "\n".join([l['trigger'] + " - " + l.get('lesson', '')
                          for l in lessons])

    # æå–å€™é€‰
    candidates = extractor.extract_from_logs(test_log, lessons)

    # å®Œæ•´è¯„ä¼°
    eval_result = extractor.evaluate_full(candidates, ground_truth)

    # è¾“å‡º
    print("ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡ï¼š")
    metrics = eval_result['metrics']
    print(f"  å¬å›ç‡ (Recall): {metrics['recall'] * 100}%")
    print(f"  ç²¾ç¡®ç‡ (Precision): {metrics['precision'] * 100}%")
    print(f"  F1 åˆ†æ•°: {metrics['f1']:.3f}")
    print(f"  è¯¯æŠ¥ç‡ (FPR): {metrics['false_positive_rate'] * 100}%")
    print(f"\n  çœŸæ­£ä¾‹ (TP): {metrics['tp']}")
    print(f"  å‡æ­£ä¾‹ (FP): {metrics['fp']}")
    print(f"  å‡è´Ÿä¾‹ (FN): {metrics['fn']}")

    print("\nğŸ“Š è¯„åˆ†åˆ†å¸ƒï¼š")
    dist = eval_result['score_distribution']
    print(f"  æœ€å°å€¼: {dist['min']}")
    print(f"  æœ€å¤§å€¼: {dist['max']}")
    print(f"  å¹³å‡å€¼: {dist['avg']:.3f}")
    print(f"  æ•°é‡: {dist['count']}")

    # ä¿å­˜
    os.makedirs('ai-collab-log/reports', exist_ok=True)
    with open('ai-collab-log/reports/phase1_full_metrics.json', 'w') as f:
        json.dump(eval_result, f, indent=2, ensure_ascii=False)
    print("\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ° ai-collab-log/reports/phase1_full_metrics.json")

if __name__ == '__main__':
    main()
