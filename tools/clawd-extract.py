#!/usr/bin/env python3
"""
Clawd Extract - ç»ˆç«¯ç‰ˆæ•°æ®æå–å·¥å…· + GLM AI åˆ†æ + é€Ÿç‡é™åˆ¶ + ç¼“å­˜

ç”¨æ³•:
  clawd-extract.py --url URL --type TYPE [--analyze] [--prompt PROMPT]

åŠŸèƒ½:
  - æå–é¡µé¢å†…å®¹
  - æå–é“¾æ¥
  - æå–å›¾ç‰‡
  - è‡ªå®šä¹‰é€‰æ‹©å™¨
  - GLM AI åˆ†æ
  - æœ¬åœ°ç¼“å­˜
  - é€Ÿç‡é™åˆ¶

ç¯å¢ƒå˜é‡:
  GLM_API_KEY - GLM API Key (ä» ~/.clawd-glm/clawdbot.json è¯»å–)
"""

import argparse
import json
import sys
import os
import re
import time
import hashlib
from urllib.request import urlopen, Request
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime, timedelta

try:
    from bs4 import BeautifulSoup
    import requests
except ImportError:
    print("âš ï¸  éœ€è¦: pip3 install beautifulsoup4 requests")
    sys.exit(1)

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨ - æ¯åˆ†é’Ÿæœ€å¤š3æ¬¡è°ƒç”¨"""
    def __init__(self, max_calls=3, period=60):
        self.max_calls = max_calls
        self.period = period
        self.calls = []
        self.cache_file = Path.home() / '.clawd-glm' / 'cache' / 'api_calls.json'
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)
        self.load_calls()
    
    def load_calls(self):
        """åŠ è½½å†å²è°ƒç”¨è®°å½•"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file) as f:
                    self.calls = json.load(f)
                # æ¸…ç†è¿‡æœŸè®°å½•
                cutoff = time.time() - self.period
                self.calls = [c for c in self.calls if c > cutoff]
            except:
                self.calls = []
    
    def save_calls(self):
        """ä¿å­˜è°ƒç”¨è®°å½•"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.calls, f)
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°å¯ä»¥è°ƒç”¨"""
        now = time.time()
        cutoff = now - self.period
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self.calls = [c for c in self.calls if c > cutoff]
        
        if len(self.calls) >= self.max_calls:
            # éœ€è¦ç­‰å¾…
            oldest = min(self.calls)
            wait_time = oldest + self.period - now
            if wait_time > 0:
                print(f"â³ é€Ÿç‡é™åˆ¶ï¼šç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
        
        # è®°å½•æœ¬æ¬¡è°ƒç”¨
        self.calls.append(now)
        self.save_calls()

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self):
        self.cache_dir = Path.home() / '.clawd-glm' / 'cache' / 'analysis'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_age_days = 7
    
    def get_cache_key(self, url, data_type, prompt):
        """ç”Ÿæˆç¼“å­˜key"""
        content = f"{url}|{data_type}|{prompt}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, url, data_type, prompt):
        """è·å–ç¼“å­˜"""
        key = self.get_cache_key(url, data_type, prompt)
        cache_file = self.cache_dir / f"{key}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file) as f:
                    cached = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                cached_time = datetime.fromisoformat(cached['timestamp'])
                if datetime.now() - cached_time < timedelta(days=self.max_age_days):
                    print("âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ")
                    return cached['analysis']
            except:
                pass
        
        return None
    
    def set(self, url, data_type, prompt, analysis):
        """ä¿å­˜ç¼“å­˜"""
        key = self.get_cache_key(url, data_type, prompt)
        cache_file = self.cache_dir / f"{key}.json"
        
        cached = {
            'url': url,
            'data_type': data_type,
            'prompt': prompt,
            'analysis': analysis,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(cache_file, 'w') as f:
            json.dump(cached, f, ensure_ascii=False, indent=2)

class ClawdExtract:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.api_key = self.load_api_key()
        self.api_url = "https://open.bigmodel.cn/api/coding/paas/v4/chat/completions"
        self.rate_limiter = RateLimiter(max_calls=3, period=60)
        self.cache = CacheManager()
    
    def load_api_key(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½ GLM API Key"""
        config_path = Path.home() / '.clawd-glm' / 'clawdbot.json'
        if config_path.exists():
            try:
                with open(config_path) as f:
                    config = json.load(f)
                    return config['models']['providers']['glm']['apiKey']
            except:
                pass
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–
        return os.getenv('GLM_API_KEY')
    
    def fetch(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        print(f"ğŸ“¡ æŠ“å–: {url}")
        try:
            req = Request(url, headers=self.headers)
            with urlopen(req, timeout=10) as response:
                return response.read().decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None
    
    def extract_page(self, url):
        """æå–é¡µé¢å†…å®¹"""
        html = self.fetch(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for tag in soup(['script', 'style', 'nav', 'footer']):
            tag.decompose()
        
        return [{
            'title': soup.title.string.strip() if soup.title else '',
            'url': url,
            'text': soup.get_text(separator='\n', strip=True)[:2000]
        }]
    
    def extract_links(self, url):
        """æå–æ‰€æœ‰é“¾æ¥"""
        html = self.fetch(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a in soup.find_all('a', href=True)[:50]:
            href = a['href']
            if href.startswith('http'):
                links.append({
                    'text': a.get_text(strip=True)[:100] or '[å›¾ç‰‡/ç©º]',
                    'url': href
                })
        
        return links
    
    def extract_images(self, url):
        """æå–æ‰€æœ‰å›¾ç‰‡"""
        html = self.fetch(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        images = []
        
        for img in soup.find_all('img', src=True)[:20]:
            src = img['src']
            if src.startswith('http'):
                images.append({
                    'alt': img.get('alt', '[æ— æè¿°]'),
                    'src': src
                })
        
        return images
    
    def extract_custom(self, url, selector):
        """è‡ªå®šä¹‰é€‰æ‹©å™¨æå–"""
        html = self.fetch(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        results = []
        
        selectors = [s.strip() for s in selector.split(',')]
        
        for sel in selectors:
            for tag in soup.select(sel)[:20]:
                results.append({
                    'tag': tag.name,
                    'text': tag.get_text(strip=True)[:200]
                })
        
        return results
    
    def analyze_with_glm(self, data, prompt="åˆ†æè¿™äº›å†…å®¹", url="", data_type=""):
        """ç”¨ GLM åˆ†ææå–çš„æ•°æ®"""
        if not self.api_key:
            print("âš ï¸  æœªé…ç½® GLM API Key")
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cached = self.cache.get(url, data_type, prompt)
        if cached:
            return cached
        
        # é€Ÿç‡é™åˆ¶
        self.rate_limiter.wait_if_needed()
        
        print("ğŸ¤– GLM åˆ†æä¸­...")
        
        # å°†æ•°æ®è½¬ä¸ºæ–‡æœ¬
        data_text = json.dumps(data, ensure_ascii=False, indent=2)
        
        # æœ€å¤šé‡è¯•3æ¬¡
        for attempt in range(3):
            try:
                response = requests.post(
                    self.api_url,
                    headers={
                        'Authorization': f'Bearer {self.api_key}',
                        'Content-Type': 'application/json'
                    },
                    json={
                        'model': 'glm-4-flash',
                        'messages': [
                            {
                                'role': 'system',
                                'content': 'ä½ æ˜¯æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡åˆ†ææå–çš„æ•°æ®ã€‚'
                            },
                            {
                                'role': 'user',
                                'content': f'{prompt}:\n\n{data_text}'
                            }
                        ],
                        'temperature': 0.7
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    analysis = result['choices'][0]['message']['content']
                    
                    # ä¿å­˜ç¼“å­˜
                    self.cache.set(url, data_type, prompt, analysis)
                    
                    return analysis
                elif response.status_code == 429:
                    # Rate limit - ç­‰å¾…åé‡è¯•
                    wait_time = (attempt + 1) * 10
                    print(f"âš ï¸  API é™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âŒ API é”™è¯¯: {response.status_code}")
                    print(f"å“åº”: {response.text}")
                    return None
                    
            except Exception as e:
                print(f"âŒ åˆ†æå¤±è´¥: {e}")
                if attempt < 2:
                    print("é‡è¯•ä¸­...")
                    time.sleep(5)
                    continue
                return None
        
        print("âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")
        return None
    
    def save(self, data, output='json', analysis=None):
        """ä¿å­˜ç»“æœ"""
        if output == 'json':
            result = {
                'data': data,
                'count': len(data),
                'timestamp': datetime.now().isoformat()
            }
            if analysis:
                result['analysis'] = analysis
            print(json.dumps(result, indent=2, ensure_ascii=False))
        elif output == 'csv':
            if not data:
                return
            
            keys = data[0].keys()
            print(','.join(keys))
            for item in data:
                print(','.join(f'"{item.get(k, "")}"' for k in keys))
            
            if analysis:
                print(f"\n\n# GLM åˆ†æ:\n{analysis}", file=sys.stderr)

def main():
    parser = argparse.ArgumentParser(description='Clawd Extract - ç»ˆç«¯ç‰ˆæ•°æ®æå–å·¥å…·')
    parser.add_argument('--url', required=True, help='ç›®æ ‡URL')
    parser.add_argument('--type', choices=['page', 'links', 'images', 'custom'], 
                       default='page', help='æå–ç±»å‹')
    parser.add_argument('--selector', help='è‡ªå®šä¹‰é€‰æ‹©å™¨ï¼ˆCSSï¼‰')
    parser.add_argument('--output', choices=['json', 'csv'], default='json', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--analyze', action='store_true', help='ç”¨ GLM AI åˆ†æ')
    parser.add_argument('--prompt', default='åˆ†æè¿™äº›å†…å®¹ï¼Œæ€»ç»“å…³é”®ç‚¹', help='åˆ†ææç¤ºè¯')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…é™¤ç¼“å­˜')
    
    args = parser.parse_args()
    
    # æ¸…é™¤ç¼“å­˜
    if args.clear_cache:
        cache_dir = Path.home() / '.clawd-glm' / 'cache' / 'analysis'
        if cache_dir.exists():
            import shutil
            shutil.rmtree(cache_dir)
            print("âœ… ç¼“å­˜å·²æ¸…é™¤")
        return
    
    extractor = ClawdExtract()
    
    # æå–æ•°æ®
    if args.type == 'page':
        data = extractor.extract_page(args.url)
    elif args.type == 'links':
        data = extractor.extract_links(args.url)
    elif args.type == 'images':
        data = extractor.extract_images(args.url)
    elif args.type == 'custom':
        if not args.selector:
            print("âŒ custom ç±»å‹éœ€è¦ --selector å‚æ•°")
            sys.exit(1)
        data = extractor.extract_custom(args.url, args.selector)
    
    # AI åˆ†æ
    analysis = None
    if args.analyze and data:
        analysis = extractor.analyze_with_glm(data, args.prompt, args.url, args.type)
    
    # ä¿å­˜ç»“æœ
    extractor.save(data, args.output, analysis)
    
    print(f"\nâœ… æå–å®Œæˆ: {len(data)} æ¡æ•°æ®", file=sys.stderr)
    if analysis:
        print(f"âœ… AI åˆ†æå®Œæˆ", file=sys.stderr)

if __name__ == '__main__':
    main()
